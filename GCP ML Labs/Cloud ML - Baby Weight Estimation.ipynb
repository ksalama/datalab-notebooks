{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured data prediction using Cloud ML Engine \n",
    "\n",
    "This notebook illustrates:\n",
    "\n",
    "1. Exploring a BigQuery dataset using Datalab\n",
    "2. Creating datasets for Machine Learning using Dataflow\n",
    "3. Creating a model using the high-level Estimator API \n",
    "4. Training on Cloud ML Engine\n",
    "5. Deploying the model\n",
    "6. Predicting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUCKET = 'ksalama-gcs-cloudml'\n",
    "PROJECT = 'ksalama-gcp-playground'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gcs_data_dir = 'gs://{0}/data/babyweight/'.format(BUCKET)\n",
    "gcs_model_dir = 'gs://{0}/ml-models/babyweight/'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -rf gs://ksalama-gcs-cloudml/ml-models/babyweight_estimator/*\n",
    "gsutil -m rm -rf gs://ksalama-gcs-cloudml/data/babyweight/big_data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query data in BigQuery\n",
    "\n",
    "The data is natality data (record of births in the US). My goal is to predict the baby's weight given a number of factors about the pregnancy and the baby's mother.  Later, we will want to split the data into training and eval datasets. The hash of the year-month will be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name data\n",
    "\n",
    "SELECT\n",
    "  CAST(mother_race AS string) race_index,\n",
    "  AVG(weight_pounds) avg_weight,\n",
    "  COUNT(weight_pounds) instance_Count\n",
    "FROM\n",
    "  `publicdata.samples.natality`\n",
    "WHERE \n",
    "    year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "AND mother_race is not null\n",
    "GROUP BY\n",
    "  mother_race\n",
    "ORDER BY\n",
    "  avg_weight DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise with Datalab commands \n",
    "http://googledatalab.github.io/pydatalab/google.datalab%20Commands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%chart columns --data data --fields race_index,avg_weight\n",
    "title: Mother Race Index vs Average Baby Weight\n",
    "height: 400\n",
    "width: 900\n",
    "hAxis:\n",
    "  title: Race Index\n",
    "vAxis:\n",
    "  title: Average Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from BigQuery as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql --module query \n",
    "\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  mother_race,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  mother_married,\n",
    "  ever_born,\n",
    "  cigarette_use,\n",
    "  alcohol_use\n",
    "FROM\n",
    "  `publicdata.samples.natality`\n",
    "WHERE \n",
    "    year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "AND\n",
    "  MOD(ABS(FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING)))), 2) = 0\n",
    "LIMIT $STEP_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import sys\n",
    "data = bq.Query(query, STEP_SIZE = step_size).to_dataframe(dialect='standard')\n",
    "print('Row count:{}'.format(len(data)))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore & Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "#plt.figure(figsize=(45, 25))\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# Baby Weight Distribution\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Baby Weight Histogram\")\n",
    "plt.hist(data.weight_pounds, bins=150)\n",
    "#plt.axis([0, 50, 0, 3500])\n",
    "plt.xlabel(\"Baby Weight Ranges\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# ---------------------------\n",
    "\n",
    "# Mother Age vs Baby Weight\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Mother Age vs Baby Weight\")\n",
    "plt.scatter(data.mother_age,data.weight_pounds)\n",
    "plt.xlabel(\"Mother Age\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "# ---------------------------\n",
    "\n",
    "# Gestation Weeks vs Baby Weight\n",
    "plt.subplot(2,3,3)\n",
    "fit = np.polyfit(data.gestation_weeks,data.weight_pounds, deg=1)\n",
    "plt.plot(data.gestation_weeks, fit[0] * data.gestation_weeks + fit[1], color='red')\n",
    "plt.scatter(data.gestation_weeks, data.weight_pounds)\n",
    "plt.xlabel(\"Gestation Weeks\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "\n",
    "#---------------------------\n",
    "\n",
    "# Is Male vs Baby Weight Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Is Male vs Baby Weight\")\n",
    "\n",
    "is_male_values = list(data.is_male.value_counts().index.values)\n",
    "is_male_data = []\n",
    "for i in is_male_values:\n",
    "    is_male_data = is_male_data + [data.weight_pounds[data.is_male == i].values]\n",
    "\n",
    "plt.boxplot(is_male_data)\n",
    "plt.axis([0, 3, 4, 11])\n",
    "plt.xlabel(\"Is Male\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "# ---------------------------\n",
    "\n",
    "# Mother Race vs Baby Weight Boxplot\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Mother Race vs Baby Weight\")\n",
    "\n",
    "race_values = list(data.mother_race.value_counts().index.values)\n",
    "race_data = []\n",
    "for i in race_values:\n",
    "    race_data = race_data + [data.weight_pounds[data.mother_race == i].values]\n",
    "\n",
    "plt.boxplot(race_data)\n",
    "plt.axis([0, 16, 4, 11])\n",
    "plt.xlabel(\"Mother Race\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "\n",
    "# # ---------------------------\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Alcohol & Cigarette Use\")\n",
    "\n",
    "alch_use_values = list(data.alcohol_use.value_counts().index.values)\n",
    "cig_use_values = list(data.cigarette_use.value_counts().index.values)\n",
    "\n",
    "use_data = []\n",
    "labels = []\n",
    "\n",
    "for i in alch_use_values:\n",
    "    for j in cig_use_values:\n",
    "        labels = labels + ['alch-use:{} & cig-use:{}'.format(i,j)]\n",
    "        condition = (data.alcohol_use == i) & (data.cigarette_use == j)\n",
    "        values = data.weight_pounds[condition].values\n",
    "        if (len(values) > 0):\n",
    "            use_data = use_data + [len(values)]\n",
    "\n",
    "plt.pie(use_data)\n",
    "plt.legend(labels, loc=\"lower center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Weight as a Baseline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_weight = data.weight_pounds.mean()\n",
    "print(\"Average Weight: {}\".format(round(avg_weight,3)))\n",
    "rmse = np.sqrt(data.weight_pounds.map(lambda value: (value-avg_weight)**2).mean())\n",
    "print(\"RMSE: {}\".format(round(rmse,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML dataset using Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "\n",
    "dataset_size = 100000\n",
    "train_size = dataset_size * 0.7\n",
    "eval_size = dataset_size * 0.3\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT\n",
    "          weight_pounds,\n",
    "          is_male,\n",
    "          mother_age,\n",
    "          mother_race,\n",
    "          plurality,\n",
    "          gestation_weeks,\n",
    "          mother_married,\n",
    "          ever_born,\n",
    "          cigarette_use,\n",
    "          alcohol_use,\n",
    "          FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
    "        FROM\n",
    "          publicdata.samples.natality\n",
    "        WHERE year > 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "        AND month > 0\n",
    "    \"\"\"\n",
    "\n",
    "out_dir = gcs_data_dir + \"big_data\"\n",
    "\n",
    "def to_csv(rowdict):\n",
    "    # pull columns from BQ and create a line\n",
    "    import hashlib\n",
    "    import copy\n",
    "    CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use'.split(',')\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = copy.deepcopy(rowdict)\n",
    "    if 'mother_race' in rowdict and rowdict['mother_race'] in races:\n",
    "        result['mother_race'] = races[rowdict['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "    \n",
    "    data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
    "    key = hashlib.sha224(data).hexdigest()  # hash the columns to form a key\n",
    "    return str('{},{}'.format(data, key))\n",
    "  \n",
    "def run_pipeline():\n",
    "    \n",
    "    job_name = 'preprocess-babyweight-data' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(out_dir, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(out_dir, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "  \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    RUNNER = 'DataflowRunner'\n",
    "  \n",
    "    pipeline = beam.Pipeline(RUNNER, options=opts)\n",
    "    \n",
    "    for step in ['train', 'eval']:\n",
    "        if step == 'train':\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(hashmonth,4) < 3 LIMIT {}'.format(query,int(train_size))\n",
    "        else:\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(hashmonth,4) = 3 LIMIT {}'.format(query,int(eval_size))\n",
    "            \n",
    "        sink_location = os.path.join(out_dir, '{}-data'.format(step))\n",
    "\n",
    "        (pipeline \n",
    "           | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "           | '{} - Process to CSV'.format(step) >> beam.Map(to_csv)\n",
    "           | '{} - Write to GCS '.format(step) >> beam.io.Write(beam.io.WriteToText(sink_location,\n",
    "                                                                file_name_suffix='csv',\n",
    "                                                                num_shards=5))\n",
    "        )\n",
    "    \n",
    "   \n",
    "    job = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Dataflow Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil ls gs://ksalama-gcs-cloudml/data/babyweight/big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Models using Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Linear Regression Model\n",
    "\n",
    "1. Define dataset metadata + input function (to read and parse the data files)\n",
    "\n",
    "2. Create feature columns based on metadata\n",
    "\n",
    "3. Instantiate the model with feature columns \n",
    "\n",
    "4. Train, evaluate, and predict using the model and the data input function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Define Metadata &  Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADER = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "TARGET_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null'], ['nokey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(tf.expand_dims(csv_row, -1), record_defaults=DEFAULTS)\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    features.pop(KEY_COLUMN)\n",
    "    target = features.pop(TARGET_NAME)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_input_fn(file_name, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=1, \n",
    "                 batch_size=500):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    file_names = tf.matching_files(file_name)\n",
    "\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Create Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "\n",
    "    is_male=tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False'])\n",
    "    mother_age=tf.feature_column.numeric_column('mother_age')\n",
    "    mother_race=tf.feature_column.categorical_column_with_vocabulary_list('mother_race', ['White', 'Black', 'American Indian', 'Chinese', \n",
    "               'Japanese', 'Hawaiian', 'Filipino', 'Unknown', 'Asian Indian', 'Korean', 'Samaon', 'Vietnamese'])\n",
    "    plurality=tf.feature_column.numeric_column('plurality')\n",
    "    gestation_weeks=tf.feature_column.numeric_column('gestation_weeks')\n",
    "    mother_married=tf.feature_column.categorical_column_with_vocabulary_list('mother_married', ['True', 'False'])\n",
    "    cigarette_use=tf.feature_column.categorical_column_with_vocabulary_list('cigarette_use', ['True', 'False', 'None'])\n",
    "    alcohol_use=tf.feature_column.categorical_column_with_vocabulary_list('alcohol_use', ['True', 'False', 'None'])\n",
    "    \n",
    "    feature_columns = [is_male, mother_age, mother_race, plurality, gestation_weeks, mother_married, cigarette_use, alcohol_use]\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Instantiate a Regression Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_model_dir = \"trained_models/babyweight_estimator_lr\"\n",
    "\n",
    "feature_columns = create_feature_columns()\n",
    "\n",
    "lr_estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns,\n",
    "                                            model_dir=local_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Train, Evaluate, and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls data/babyweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. train the model with the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "train_data_files = \"data/babyweight/train.csv\"\n",
    "\n",
    "train_input_fn = lambda: csv_input_fn(train_data_files, \n",
    "                                              mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                              num_epochs=10,\n",
    "                                              batch_size = 200\n",
    "                                         )\n",
    "\n",
    "# remove the following line of code to resume training\n",
    "shutil.rmtree(local_model_dir, ignore_errors=True)\n",
    "\n",
    "lr_estimator.train(train_input_fn, max_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls trained_models/babyweight_estimator_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_data_files = \"data/babyweight/eval.csv\"\n",
    "\n",
    "eval_input_fn =lambda: csv_input_fn(eval_data_files)\n",
    "\n",
    "lr_estimator.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predictions = lr_estimator.predict(eval_input_fn)\n",
    "values = list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))\n",
    "print(\"\")\n",
    "print(\"Predicted Values: {}\".format(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a DNN Liner Combined Regression Model + Feature Engineering\n",
    "\n",
    "1. Define dataset metadata + input function (to read and parse the data files, + **process features**) \n",
    "\n",
    "2. Create feature columns based on metadata + **Extended Feature Columns**\n",
    "\n",
    "3. Initialise the Estimator + **Wide & Deep Columns for the combined DNN model**\n",
    "\n",
    "4. Run **train_and_evaluate** experiment: Supply TrainSpec, EvalSepc, config, and params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define input function with process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADER = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "TARGET_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null'], ['nokey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(tf.expand_dims(csv_row, -1), record_defaults=DEFAULTS)\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    features.pop(KEY_COLUMN)\n",
    "    target = features.pop(TARGET_NAME)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be applied in traing and serving\n",
    "def process_features(features):\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_input_fn(file_name, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=1, \n",
    "                 batch_size=500):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    file_names = tf.matching_files(file_name)\n",
    "\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row))\n",
    "    dataset = dataset.map(lambda features, target: (process_features(features), target))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Feature Columns with Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deep_and_wide_columns():\n",
    "\n",
    "    is_male=tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False'])\n",
    "    mother_age=tf.feature_column.numeric_column('mother_age')\n",
    "    mother_race=tf.feature_column.categorical_column_with_vocabulary_list('mother_race', ['White', 'Black', 'American Indian', 'Chinese', \n",
    "               'Japanese', 'Hawaiian', 'Filipino', 'Unknown', 'Asian Indian', 'Korean', 'Samaon', 'Vietnamese'])\n",
    "    plurality=tf.feature_column.numeric_column('plurality')\n",
    "    gestation_weeks=tf.feature_column.numeric_column('gestation_weeks')\n",
    "    mother_married=tf.feature_column.categorical_column_with_vocabulary_list('mother_married', ['True', 'False'])\n",
    "    cigarette_use=tf.feature_column.categorical_column_with_vocabulary_list('cigarette_use', ['True', 'False', 'None'])\n",
    "    alcohol_use=tf.feature_column.categorical_column_with_vocabulary_list('alcohol_use', ['True', 'False', 'None'])\n",
    "    \n",
    "    # extended feature columns\n",
    "    cigarette_use_X_alcohol_use = tf.feature_column.crossed_column([cigarette_use, alcohol_use], 9)\n",
    "    \n",
    "    mother_age_bucketized = tf.feature_column.bucketized_column(mother_age, boundaries=[18, 22, 28, 32, 36, 40, 42, 45, 50])\n",
    "    \n",
    "    mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column( [mother_age_bucketized,mother_race],  120)\n",
    "    \n",
    "    mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(mother_race_X_mother_age_bucketized, 5)\n",
    "    \n",
    "    # wide and deep columns\n",
    "    wide_columns = [is_male, mother_race, plurality, mother_married, cigarette_use, alcohol_use, cigarette_use_X_alcohol_use, mother_age_bucketized, mother_race_X_mother_age_bucketized] \n",
    "    deep_columns = [mother_age, gestation_weeks, mother_race_X_mother_age_bucketized_embedded]\n",
    "    \n",
    "    return wide_columns, deep_columns\n",
    "\n",
    "#get_deep_and_wide_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Create a DNN Regression Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_DNNLinearCombinedRegressor(run_config, hparams):\n",
    "  \n",
    "    wide_columns, deep_columns = get_deep_and_wide_columns()\n",
    "\n",
    "    dnn_optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "                linear_feature_columns = wide_columns,\n",
    "                dnn_feature_columns = deep_columns,\n",
    "                dnn_optimizer=dnn_optimizer,\n",
    "                dnn_hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Local Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. RunConfig and Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "hparams  = tf.contrib.training.HParams(num_epochs = 10,\n",
    "                                       batch_size = 500,\n",
    "                                       hidden_units=[32, 16],\n",
    "                                       max_steps = 100,\n",
    "                                       learning_rate = 0.1,\n",
    "                                       evaluate_after_sec=10)\n",
    "\n",
    "# RunConfig\n",
    "local_model_dir = \"trained_models/babyweight_estimator_dnn\"\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=local_model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_serving_input_fn():\n",
    "  \n",
    "  SERVING_HEADER = 'is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use'.split(',')\n",
    "  SERVING_HEADER_DEFAULTS = [['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null']]\n",
    "\n",
    "  rows_string_tensor = tf.placeholder(dtype=tf.string,\n",
    "                                         shape=[None],\n",
    "                                         name='csv_rows')\n",
    "    \n",
    "  receiver_tensor = {'csv_rows': rows_string_tensor}\n",
    "\n",
    "  row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "  columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "  features = dict(zip(SERVING_HEADER, columns))\n",
    "  \n",
    "  # apply feature preprocessing used input_fn\n",
    "  features = process_features(features)\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "        features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. TrainSpec and EvalSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_files = \"data/babyweight/train.csv\"\n",
    "eval_data_files = \"data/babyweight/eval.csv\"\n",
    "\n",
    "# TrainSpec\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn = lambda: csv_input_fn(\n",
    "    train_data_files,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "    num_epochs= hparams.num_epochs,\n",
    "    batch_size = hparams.batch_size\n",
    "  ),\n",
    "  max_steps=hparams.max_steps,\n",
    ")\n",
    "\n",
    "# EvalSpec\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn =lambda: csv_input_fn(eval_data_files),\n",
    "  exporters=[tf.estimator.LatestExporter(\n",
    "      name=\"estimate\",  # the name of the folder in which the model will be exported to under export\n",
    "      serving_input_receiver_fn=csv_serving_input_fn,\n",
    "      exports_to_keep=1,\n",
    "      as_text=True)],\n",
    "  steps = None,\n",
    "  throttle_secs = hparams.evaluate_after_sec # evalute after each 10 training seconds!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# remove the following line of code to resume training\n",
    "shutil.rmtree(local_model_dir, ignore_errors=True)\n",
    "\n",
    "dnn_estimator = create_DNNLinearCombinedRegressor(run_config, hparams)\n",
    "\n",
    "# run train and evaluate experiment\n",
    "tf.estimator.train_and_evaluate(\n",
    "  dnn_estimator,\n",
    "  train_spec,\n",
    "  eval_spec\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls trained_models/babyweight_estimator_dnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(\"trained_models/babyweight_estimator\")\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to stop TensorBoard\n",
    "# TensorBoard().stop(23002)\n",
    "# print('stopped TensorBoard')\n",
    "# TensorBoard().list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m cp -r gs://ksalama-gcs-cloudml/ml-packages/babyweight ml-packages\n",
    "ls ml-packages/babyweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=BASIC # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-data.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-data.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/${MODEL_NAME}\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --scale-tier=${TIER} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=500 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=500 \\\n",
    "        --learning-rate=0.01 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Cloud ML Engine + GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=BASIC_GPU # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-*.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-*.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/${MODEL_NAME}_${TIER}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --scale-tier=${TIER} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=10 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.01 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Cloud ML Engine + Custom GPUs Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=CUSTOM # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/big_data/train-*.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/big_data/eval-*.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/${MODEL_NAME}_${TIER}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        --config=ml-packages/babyweight-tf1.4/custom.yaml \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.001 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters Tuning on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/big_data/train-*.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/big_data/eval-*.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/${MODEL_NAME}_tune\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=tune_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        --config=ml-packages/babyweight-tf1.4/hyperparams.yaml \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=europe-west1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "\n",
    "#MODEL_BINARIES=$(gsutil ls gs://${BUCKET}/models/${MODEL_NAME}/export/Servo | tail -1)\n",
    "\n",
    "#gsutil ls ${MODEL_BINARIES}\n",
    "\n",
    "# delete model version\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model=${MODEL_NAME}\n",
    "\n",
    "# delete model\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "\n",
    "# deploy model to GCP\n",
    "#gcloud ml-engine models create ${MODEL_NAME} --regions=${REGION}\n",
    "\n",
    "#deploy model version\n",
    "#gcloud ml-engine versions create ${MODEL_VERSION} --model=${MODEL_NAME} --origin=${MODEL_BINARIES} --runtime-version=1.4\n",
    "\n",
    "#echo  ${MODEL_NAME} ${MODEL_VERSION} \n",
    "# invoke deployed model to make prediction given new data instances\n",
    "#gcloud ml-engine predict --model=${MODEL_NAME} --version=${MODEL_VERSION} --json-instances=data/babyweight/new-data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume the Model as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "def estimate(project, model_name, version, instances):\n",
    "\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "                discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "    request_data = {'instances': instances}\n",
    "\n",
    "    model_url = 'projects/{}/models/{}/versions/{}'.format(project, model_name, version)\n",
    "    response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "\n",
    "    estimates = list(map(lambda item: round(item[\"scores\"],2)\n",
    "        ,response[\"predictions\"]\n",
    "    ))\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT='ksalama-gcp-playground'\n",
    "MODEL_NAME='babyweight_estimator'\n",
    "VERSION='v1'\n",
    "\n",
    "instances =  [\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'False',\n",
    "        'mother_age': 29.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 38,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 2.0,\n",
    "        'gestation_weeks': 37,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'True'\n",
    "      }\n",
    "  ]\n",
    "\n",
    "estimates = estimate(instances=instances\n",
    "                     ,project=PROJECT\n",
    "                     ,model_name=MODEL_NAME\n",
    "                     ,version=VERSION)\n",
    "\n",
    "print(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the end ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
